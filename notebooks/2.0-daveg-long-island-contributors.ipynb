{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Large Files\n",
    "\n",
    "We now have a list of Long Island zipcodes, and we can use it to make sure we are only looking at Long Island data. There's an awful lot of data, though - 18GB, in 54 separate files. We could read each file separately into Pandas data frames, and then filter them to the Long Island zipcodes. But, there is a better way! We can use Polars, a new dataframe library built on Rust for speed and performance. We rely on some neat features along the way to write less code and get more for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the header file description from the FEC to figure out the columns and their order. This lets us see, for instance, that STATE is column 10 and ZIP_CODE is column 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CMTE_ID\n",
      "2 AMNDT_IND\n",
      "3 RPT_TP\n",
      "4 TRANSACTION_PGI\n",
      "5 IMAGE_NUM\n",
      "6 TRANSACTION_TP\n",
      "7 ENTITY_TP\n",
      "8 NAME\n",
      "9 CITY\n",
      "10 STATE\n",
      "11 ZIP_CODE\n",
      "12 EMPLOYER\n",
      "13 OCCUPATION\n",
      "14 TRANSACTION_DT\n",
      "15 TRANSACTION_AMT\n",
      "16 OTHER_ID\n",
      "17 TRAN_ID\n",
      "18 FILE_NUM\n",
      "19 MEMO_CD\n",
      "20 MEMO_TEXT\n",
      "21 SUB_ID\n",
      "\n"
     ]
    }
   ],
   "source": [
    "header_columns = \"\"\"CMTE_ID,AMNDT_IND,RPT_TP,TRANSACTION_PGI,IMAGE_NUM,TRANSACTION_TP,ENTITY_TP,NAME,CITY,STATE,ZIP_CODE,EMPLOYER,OCCUPATION,TRANSACTION_DT,TRANSACTION_AMT,OTHER_ID,TRAN_ID,FILE_NUM,MEMO_CD,MEMO_TEXT,SUB_ID\n",
    "\"\"\".split(\",\")\n",
    "for i, c in enumerate(header_columns):\n",
    "    print(i+1, c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the list of zipcodes we created in our last notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "li_zip_codes = gpd.read_file(\"../references/long_island_zipcodes.geojson\")\n",
    "zip_codes = li_zip_codes.postalcode.unique().tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we can't just apply our header columns to the data. As we might expect from such a large set of data, some of it is a mess. Surprises I found while trying to process the data: \n",
    "\n",
    "- Non-US zipcodes like 'W11 3LN'. This is a postal code in England (!!!!!)\n",
    "- Bad tranaction_dt 'LINCOLN'\n",
    "- Records missing some of the required columns\n",
    "\n",
    "That last one is the most serious because, if we provide a list of columns to polars, it forces every record to have the same number of fields and fails when that's not true. But, we can refer to the columns by their positions instead, treat every column as utf8, and then apply filters to the data to make sure we only get the zipcodes we want. We also tell polars to scan the data in chunks, rather than try to read the entire file into RAM. This code is unbelievably performant - it scanned 18GB of data from my machine in just over a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STREAMING CHUNK SIZE: 2380 rows\n"
     ]
    }
   ],
   "source": [
    "fec_df = pl.scan_csv(\n",
    "    \"../data/raw/by_date/*.txt\", \n",
    "    separator='|', \n",
    "    has_header=False, \n",
    "    infer_schema_length=0,\n",
    "    ignore_errors=True,\n",
    "    missing_utf8_is_empty_string=True,\n",
    ").filter(pl.col(\"column_10\") == \"NY\").filter(pl.col(\"column_11\").str.slice(0,5).is_in(zip_codes)).collect(streaming=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a subset of the data, we can apply our header column to it so we can get the names we want to work with, which were taken from here:\n",
    "\n",
    "https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec_df.columns = header_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the file to parquet, which will let us use it with Pandas in our next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec_df.write_parquet(\"../data/processed/individual_2020_li.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df910395cc413df7aae2ff0bdd27dad0b30cac9664131e33d846155d5f5a977f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
